# Machine learning, Data Science and Deep learning

ðŸ“š Program Structure

## Python Programming Foundations

- Python basics, data types, control flow, functions, OOP
- Advanced Python: file handling, exceptions, decorators, iterators, generators
- Data structures: lists, tuples, sets, dictionaries

## Data Analysis with Numpy & Pandas

- Numpy: arrays, operations, indexing, reshaping, broadcasting
- Pandas: Series, DataFrames, cleaning, merging, grouping
- Visualization: Matplotlib, Seaborn, Plotly, Dash, Streamlit

## Data Analysis Process & SQL

- Data gathering, cleaning, EDA, feature engineering
- SQL: CRUD, joins, subqueries, window functions, database design
- Tableau: dashboards and visualization

## Statistics, Probability, and Linear Algebra

- Descriptive & inferential statistics, probability distributions, hypothesis testing
- Linear algebra: vectors, matrices, transformations, eigenvalues/vectors

## Machine Learning Foundations

- Regression: linear, polynomial, regularization (Ridge, Lasso, ElasticNet)
- Feature selection & engineering
- Model evaluation: metrics, cross-validation, hyperparameter tuning

## Classification & Advanced ML

- Naive Bayes, Logistic Regression, SVM
- Decision Trees, Random Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost
- Clustering: KMeans, DBSCAN, GMM, hierarchical, t-SNE, LDA

## Real-World Projects & Capstone

- End-to-end projects: data gathering, cleaning, EDA, modeling, deployment
- Capstone: Real estate price prediction, analytics, recommender system, AWS deployment

## MLOps Curriculum

- MLOps foundations: version control, reproducibility, ML pipelines, experiment tracking
- Deployment: Docker, Kubernetes, AWS Sagemaker, CI/CD, monitoring, scaling
- Advanced: DAGs (Airflow, Kubeflow), technical debt, distributed computing

## Artificial Neural Networks (ANN)

- Biological inspiration: neuron structure, synapses, artificial neurons
- History: perceptron, backpropagation, MLPs, deep learning resurgence
- Perceptron & MLPs: limitations, XOR problem, hidden layers
- Layers: input, hidden, output
- Activation functions: sigmoid, tanh, ReLU, Leaky ReLU, softmax
- Forward propagation: computations, outputs
- Loss functions: MSE, cross-entropy, hinge loss
- Backpropagation: chain rule, gradients, computational graphs
- Gradient descent variants: batch, SGD, mini-batch
- Optimization: momentum, Nesterov, AdaGrad, RMSProp, Adam
- Regularization: L1/L2, dropout, early stopping
- Hyperparameter tuning: learning rate, batch size, epochs, architecture, grid/random/Bayesian search
- Vanishing/exploding gradients: problems & solutions (initialization, ReLU)
- Weight initialization: Xavier, He
- Batch normalization

## Convolutional Neural Networks (CNN)

- MLP challenges for images: dimensionality, spatial invariance
- CNN advantages: parameter sharing, local connectivity
- Convolution: kernels, filters, stride, padding
- Activation: ReLU, Leaky ReLU, ELU
- Pooling: max, average, size/stride
- Fully connected layers, flattening
- Loss functions: cross-entropy, MSE
- CNN architectures: LeNet-5, AlexNet, VGG, Inception, ResNet, MobileNets
- Preprocessing: normalization, augmentation (rotation, flipping, cropping, noise)
- Transfer learning: pre-trained models, fine-tuning, feature extraction
- Object detection: R-CNN, Fast/Faster R-CNN, YOLO, SSD, Mask R-CNN
- Segmentation: FCN, U-Net
- Generative models: autoencoders, VAEs, GANs (DCGAN)

## Recurrent Neural Networks (RNN)

- RNN architecture: sequential data, basic structure, activation
- Forward/backpropagation through time (BPTT)
- Training challenges: vanishing/exploding gradients, solutions (clipping, LSTM, GRU)
- LSTM & GRU: gates, intuition, comparison
- Deep/bidirectional RNNs: stacking, residuals, regularization
- Applications: language modeling, sentiment analysis, POS tagging, time series

## Sequence-to-Sequence & Attention

- Encoder-decoder: architecture, variable-length sequences, loss, teacher forcing
- Attention: motivation, additive (Bahdanau), multiplicative (Luong), implementation, visualization

## Transformers & NLP

- Transformer architecture: self-attention, positional encoding, feedforward, normalization, residuals
- Encoder-decoder stacks, implementation steps
- Transformer types: BERT, GPT, RoBERTa, ALBERT, T5
- Fine-tuning: loading, modifying, hyperparameters, best practices
- Pre-training: MLM, CLM, data, tokenization, distributed/mixed precision training
- Optimization: efficient attention, compression (quantization, pruning, distillation), hardware/software tools
- NLP applications: classification, QA, translation, summarization, generation, NE
